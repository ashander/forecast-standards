---
title: "Example metadata for a Lotka-Volterra population growth forecast"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Logistic Growth Simulation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, message=FALSE}
library(EML)
library(ncdf4)
library(emld)
library(lubridate)
library(tibble)
library(dplyr)
library(tidyr)
library(reshape2)
emld::eml_version("eml-2.2.0")
set.seed(42)
```

A simple, example forecast of population growth of two interacting species

========================================================

# Generating the forecast

To illustrate the application of the forecast standard, we'll consider the classic Lotka-Volterra population growth model. To keep this first example simple, we'll only consider two species and two uncertainties - an additive process error, which converts this to a stochastic Lotka-Volterra, and an observation error. We'll propagate uncertainty by running multiple ensembles. To illustrate the ability of the output format to accommodate spatial dimensions, we'll run the model at multiple depths in a water column (e.g. lake or ocean) but those depths are not interacting -- we know this isn't realistic, but we want to keep things simple. Overall, this gives us the following *dimensions*

```{r}
forecast_issue_time = as.Date("2001-03-04")  ## start date of our forecast
n_time <- 30
times = seq(from=forecast_issue_time,by="1 day",length=n_time)

depths <- c(1, 3, 5)
n_depths <- length(depths)

n_ensembles <- 10
ensembles <- seq_len(n_ensembles)

species <- c("species_1","species_2")
n_species <- length(species)

obs_dim <- 2 ## 1 = latent state
             ## 2 = latent state + observation error
```

Next we're going to assume fixed parameters and initial conditions, set up storage for our forecast output (which we see has 5 dimensions), and then forward simulate the populations

```{r}
## parameters for species_1 and species_2
r <- c(1, 3)
K <-  c(10, 20)
alpha <-  c(0.2, 0.3)

## process error
process_sd <- 0.01

## observation error
obs_sd <- c(0.5,0.2)

## initial conditions
n0 <-  0.5

## forecast output storage
n <- array(NA,dim = c(n_time, n_depths, n_ensembles, obs_dim,2)) ## last dim is species
n[1,,,,] <- n0

## forecast flag
forecast <- rep(0, n_time) ## this code indicated a hindcast

## data assimilation flag
data_assimilation <- rep(0, n_time)  ## this code indicates a 'free-run' that didn't assimilate new data

## forward simulation
for(t in 2:n_time){
  for(depth in 1:n_depths){
    for(ens in 1:n_ensembles){
      ## predict latent state
      n_curr <- n[t-1,depth,ens,1,] ## current state
      n[t, depth, ens,1,1] <-  n_curr[1] + 
          r[1]*n_curr[1]*(1-((n_curr[1] + 
          alpha[1]*n_curr[2])/K[1])) + rnorm(1, 0, process_sd)
      n[t, depth, ens,1,2] <-  n_curr[2] + 
          r[2]*n_curr[2]*(1-((n_curr[2] + 
          alpha[2]*n[1])/K[2])) + rnorm(1, 0, process_sd)
      ## predict observed values
      n[t,depth,ens,2,] = rnorm(2,n[t,depth,ens,1,],obs_sd)
    }
  }
}
```

Here's a simple visualization of our ensemble forecast at one depth

```{r}
plot(times,n[,1,1,1,1],ylim=range(n),ylab="Population Density",type='l')
for(s in seq_along(species)){ ##species
  for(e in ensembles){
    lines(times,n[,1,e,1,s],col=s)                                  ## latent
    points(times+runif(n_time,-0.12,0.12),n[,1,e,2,s],col=s,cex=0.35)  ## pseudo-obs w/ jitter
  }
}
legend("bottomright",legend=species,col = seq_along(species),lty = 1)
```

# Saving to a standardized output format

## Standard Option 1: netCDF

### Forecast identifiers

Because netCDF is a self-documenting format we're going to start by defining some forecast 
identifiers (which we'll also later use in the metadata)

`forecast_project_id` identifies forecasts coming from a single project and is used to link 
forecasts different model versions. Examples might include a project Github repository or a 
team name.

`forecast_model_id` is updated each time the forecast code is modified (e.g. model structure, 
model calibration, forecast workflow).  Examples might be a DOI, version number, or Github 
hash. Results from a single forecast_model_id are considered comparable.

`forecast_iteration_id` represents a unique ID for each forecast run. Examples might be a
start time or database ID.

For example, if you have a forecast code base on GitHub and launch a forecast from that code 
that runs daily for 365 days, then there will be one forecast_project_id and 365 
forecast_iteration_ids. A paper analyzing the forecasts would cite the forecast_project_id.

``` {r}
forecast_project_id <- "LogisticDemo"
forecast_model_id   <- "v0.3"
forecast_iteration_id <- "20010304T060000"   # ISO datetime should make a valid Forecast_id
```

### netcdf Dimensions

Once we have our ID's defined, we're going to use `ncdim_def` to define the dimensions that our output variables will have

```{r}
## Set dimensions
timedim <- ncdim_def("time",   ## dimension name
                     units = paste('days since',forecast_issue_time),
                               ## size of timestep, with units and start date
                     vals = as.numeric(times - forecast_issue_time),
                               ## sequence of values that defines the dimension.
                               ## netCDF expresses time dimensions relative to a
                               ## specified start date, in this case forecast_issue_time
                     longname = 'time')
                               ## descriptive name

depthdim <- ncdim_def("depth", 
                      units = "meters",
                      vals = depths, 
                      longname = 'Depth from surface') 

ensdim <- ncdim_def("ensemble",             
                    units = "",
                    vals = ensembles,       
                    longname = 'ensemble member') 

obsdim <- ncdim_def("obs_flag",
                    units = "",
                    vals = 1:obs_dim,
                    longname = "observation error flag. 1 = latent state, 2 = w/ obs error")

## quick check that units are valid
udunits2::ud.is.parseable(timedim$units)
udunits2::ud.is.parseable(depthdim$units)
udunits2::ud.is.parseable(ensdim$units)
udunits2::ud.is.parseable(obsdim$units)
```

### Variable names

Once we've defined our dimensions, we can then define the metadata about our variables using `ncvar_def`

```{r}
fillvalue <- 1e32  ## missing data value

#Define variables
def_list <- list()
def_list[[1]] <- ncvar_def(name =  "species_1",
                           units = "number of individuals",
                           dim = list(timedim, depthdim, ensdim, obsdim),
                           missval = fillvalue,
                           longname = '<scientific name of species 1>',
                           prec="single")
def_list[[2]] <- ncvar_def(name =  "species_2",
                           units = "number of individuals",
                           dim = list(timedim, depthdim, ensdim, obsdim),
                           missval = fillvalue,
                           longname = '<scientific name of species 2>',
                           prec="single")
def_list[[3]] <- ncvar_def(name =  "forecast",
                           units = "integer",
                           dim = list(timedim),
                           missval = fillvalue,
                           longname = 'EFI standard forecast code. 0 = hindcast',
                           prec="single")
def_list[[4]] <- ncvar_def(name =  "data_assimilation",
                           units = "integer",
                           dim = list(timedim),
                           missval = fillvalue,
                           longname = 'EFI standard data assimilation code. 0 = no data',
                           prec="single")
```

### Creating the file

Finally, we'll now create our netCDF file and add our outputs and global metadata to the file

```{r}

## file name
ncfname <- system.file("extdata", "logistic-forecast-ensemble-multi-variable-space-long.nc", package="EFIstandards")

## open your netCDF file
ncout <- nc_create(ncfname,def_list,force_v4=T)

## fill in our output data
ncvar_put(ncout,def_list[[1]] , n[, , , ,1 ])        ## species 1
ncvar_put(ncout,def_list[[2]] , n[, , , ,2 ])        ## species 2
ncvar_put(ncout,def_list[[3]] , forecast) ## forecast flag
ncvar_put(ncout,def_list[[4]] , data_assimilation) ## data_assimilation flag

## Global attributes (metadata)
ncatt_put(ncout,0,"forecast_project_id", as.character(forecast_project_id), 
          prec =  "text")
ncatt_put(ncout,0,"forecast_model_id",as.character(forecast_model_id), 
          prec =  "text")
ncatt_put(ncout,0,"forecast_iteration_id",as.character(forecast_iteration_id), 
          prec =  "text")

nc_close(ncout)   ## make sure to close the file
```

# Standard Option 2: ensemble CSV

Convert to a flat file format (CSV) with one column for each variable and all
ensemble members saved

```{r}
## wrangle data into a tidy format
df_combined <- reshape2::melt(n,varnames=c("time","depth","ensemble","obs_flag","species")) %>%
  pivot_wider(id_cols = 1:4,names_from = "species",names_prefix = "species_",values_from = "value") %>% 
  mutate(time = times[time]) %>%
  mutate(depth = depths[depth]) %>%
  right_join(data_frame(time=times,forecast=forecast,data_assimilation = data_assimilation))

head(df_combined)

write.csv(df_combined, 
          file = "logistic-forecast-ensemble-multi-variable-multi-depth.csv")
```

# Standard Option 3: summary CSV

Convert to a flat file format (CSV) with forecast distribution summaries saved

```{r}
df_species_1 <- df_combined %>% 
  select(-species_2) %>% 
  group_by(time, depth, data_assimilation) %>% 
  summarize(mean = mean(species_1),
            Conf_interv_02.5 = quantile(species_1, 0.025),
            Conf_interv_97.5 = quantile(species_1, 0.975)) %>% 
  pivot_longer(cols = c("mean","Conf_interv_02.5","Conf_interv_97.5"),
               names_to = "Statistic",
               values_to = "species_1")

df_species_2 <- df_combined %>% 
  select(-species_1) %>% 
  group_by(time, depth, data_assimilation) %>% 
  summarize(mean = mean(species_2),
            Conf_interv_02.5 = quantile(species_2, 0.025),
            Conf_interv_97.5 = quantile(species_2, 0.975)) %>% 
  pivot_longer(cols = c("mean","Conf_interv_02.5","Conf_interv_97.5"),
               names_to = "Statistic",
               values_to = "species_2")

 df_summary <- right_join(df_species_1, df_species_2)[,c(1,2,4,5,6,3)]
 
 df_summary
 
write.csv(df_summary, 
          file = "logistic-forecast-summary-multi-variable-multi-depth.csv")

```

# Standardized Metadata

### Forecast output entity: dataTable
Let's begin by documenting the metadata of the forecast output data table itself, which we'll do using EML's `dataTable` entity. In this example we'll assume we're working with format 2 (ensemble CSV), though most of the metadata would be identical for all three formats.

```{r}
## define variable names, units, etc
## in practice, this might be kept in a spreadsheet
attributes <- tibble::tribble(
  ~attributeName,     ~attributeDefinition,                      ~unit,                  ~formatString, ~numberType, ~definition,
  "time",              "time",                                    "year",                "YYYY-MM-DD",  "numberType", NA,
  "depth",             "depth in reservior",                      "meter",                NA,           "real",       NA,
  "ensemble",          "index of ensemble member",                "dimensionless",        NA,           "integer",    NA,
  "species_1",         "Population density of species 1",         "numberPerMeterSquared", NA,          "real",       NA,
  "species_2",         "Population density of species 2",         "numberPerMeterSquared", NA,          "real",       NA,
  "data_assimilation", "Flag whether time step assimilated data", "dimensionless",        NA,           "integer",    NA
) ## note: EML uses a different unit standard than UDUNITS. For now use EML. EFI needs to provide a custom unitList.
attributes
attrList <- set_attributes(attributes, 
                           col_classes = c("Date", "numeric", "numeric", 
                                           "numeric","numeric", "numeric"))

## sets metadata about the file itself (name, file type, size, MD5, etc)
physical <- set_physical("logistic-forecast-ensemble-multi-variable-multi-depth.csv")

## set metadata for the file as a whole
dataTable <- eml$dataTable(
                 entityName = "forecast",  ## this is a standard name to allow us to distinguish this entity from 
                 entityDescription = "Forecast of population size using a depth specific model",
                 physical = physical,
                 attributeList = attrList)
```
Here `entityName = "forecast"` is a standard name within the EFI standard to allow us to distinguish this entity from metadata about parameters, drivers, etc.

There's a lot more optional terminology that could be exploited here -- for 
instance, the specification lets us define different missing value codes (and 
explanations) for each column, and allows us to indicate `precision`, `minimum`
and `maximum`.  

Note that `physical` type can document almost any formats as well, including 
NetCDF etc.  A NetCDF file would still document the variables measured in much 
the same way regardless of the underlying representation. 

### Provenance 
Now that we've documented the actual data.frame itself, we can add additional 
metadata to the record describing our forecast, which is essential for citing, 
discovering, and interpreting the result.  We start with some authorship 
information. 

```{r}
me <- list(individualName = list(givenName = "Quinn", 
                                 surName = "Thomas"),
           electronicMailAddress = "rqthomas@vt.edu",
           id = "https://orcid.org/0000-0003-1282-7825")
```

### Coverage
Set Taxonomic, Temporal, and Geographic Coverage.  (Look, apparently we're 
modeling population densities of *Gloeotrichia echinulata* cyanobacterium in Lake Sunapee, NH, USA)

```{r}
coverage <- 
  set_coverage(begin = first(time), 
               end = last(time),
               sci_names = "Gloeotrichia echinulata",
               geographicDescription = "Lake Sunapee, NH, USA ",
               west = -72.15, east = -72.05, 
               north = 43.48, south = 43.36)
```

### Keywords
Set key words.  We will need to develop a EFI controlled vocabulary
```{r}
keywordSet <- list(
    list(
        keywordThesaurus = "EFI controlled vocabulary",
        keyword = list("forecast",
                    "population",
                    "timeseries")
    ))
```

### Abstract
Our dataset needs an abstract describing what this is all about.
```{r}
abstract_text <- system.file("extdata", "abstract.md", package="EFIstandards", mustWork = TRUE)
```

### Dataset
Next, we'll combine these various bits to document the output dataset as a whole
```{r}
dataset = eml$dataset(
               title = "A very simple Lotka-Volterra forecast",
               creator = me,
               contact = list(references="https://orcid.org/0000-0003-1282-7825"),
               pubDate = forecast_issue_time,
               intellectualRights = "http://www.lternet.edu/data/netpolicy.html.",
               abstract =  "An illustration of how we might use EML metadata to describe an ecological forecast",
               dataTable = dataTable,
               keywordSet = keywordSet,
               coverage = coverage
               )
```

## Forecast-specific additionalMetadata
The EFI standard is using EML's `additionalMetadata` capacity.

```{r}
additionalMetadata <- eml$additionalMetadata(
  metadata = list(
    forecast = list(
## Basic elements
      timestep = "1 day", ## should be udunits parsable; already in coverage -> temporalCoverage?
      forecast_horizon = "30 days",
      forecast_issue_time = forecast_issue_time,
      forecast_iteration_id = forecast_iteration_id,
      forecast_project_id = forecast_project_id,
      metadata_standard_version = "0.2",
      model_description = list(
        name = "discrete Lotka–Volterra model",
        type = "process-based",
        repository = "https://github.com/eco4cast/EFIstandards/blob/master/vignettes/logistic-metadata-example.Rmd"
      ),
## UNCERTAINTY CLASSES
      initial_conditions = list(
        # Possible values: no, contains, data_driven, propagates, assimilates
        uncertainty = "contains",
        # Number of parameters / dimensionality
        complexity = 2
      ),
      parameters = list(
        uncertainty = "contains",
        complexity = 3
      ),
      random_effects = list(
        uncertainty = "no"
      ),
      process_error = list(
        uncertainty = "propagates",
        propagation = list(
          type = "ensemble", # ensemble vs analytic
          size = 10          # required if ensemble
        ),
        complexity = 2,
        covariance = FALSE
      ),
      drivers = list(
        uncertainty = "no"
      )
    ) # forecast
  ) # metadata
) # eml$additionalMetadata
```


All we need now is to combine the dataset metadata with the forecast additionalMetadata 
```{r}
my_eml <- eml$eml(dataset = dataset,
           additionalMetadata = additionalMetadata,
           packageId = forecast_project_id, 
           system = "uuid"  ## system used to generate packageId
           )
```

Once we have finished building our EML metadata, we can confirm it is valid.  
This will catch any missing elements.  (Recall that what is 'required' depends 
on what you include -- for example, you don't have to document a `dataTable` at 
all, but if you do, you have to document the "physical" file format it is in  
(e.g. `csv`) and the attributes and units it uses!)

```{r}
## check base EML
eml_validate(my_eml)

## check that the EML is also a valid EFI forecast
EFIstandards::forecast_validator(my_eml)
```



We are now ready to write out a valid EML document: 

```{r}
write_eml(my_eml, "forecast-eml.xml")
```

At this point, we could easily upload this metadata along with the data itself 
to DataONE via the API (or `dataone` R package.)

We can also generate a JSON-LD version of EML:

```{r}
emld::as_json(as_emld("forecast-eml.xml"), file = "forecast-eml.json")
```




```{r include=FALSE}
## Cleanup
lapply(list.files(pattern = "[.]csv"), unlink)
lapply(list.files(pattern = "[.]json"), unlink)
lapply(list.files(pattern = "[.]xml"), unlink)

```
